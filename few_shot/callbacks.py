"""
Ports of Callback classes from the Keras library.
"""
from tqdm import tqdm
import numpy as np
import torch
from collections import OrderedDict, Iterable
import warnings
import os
import csv
import io
from torch.nn import Module
from torch.utils.data import DataLoader
from typing import Callable, List, Union
from few_shot.metrics import NAMED_METRICS


class CallbackList(object):
    '''
    Container abstracting a list of callbacks.
    '''

    def __init__(self, callbacks):
        self.callbacks = [c for c in callbacks]

    def set_params(self, params):
        for callback in self.callbacks:
            callback.set_params(params)

    def set_model(self, model):
        for callback in self.callbacks:
            callback.set_model(model)

    def on_epoch_begin(self, epoch, logs=None):
        '''
        Called at the start of an epoch
        :param epoch: integer, index of epoch.
        :param logs: dictionary of logs.
        '''
        logs = logs or {}
        for callback in self.callbacks:
            callback.on_epoch_begin(epoch, logs)

    def on_epoch_end(self, epoch, logs=None):
        '''
        Called at the end of an epoch
        :param epoch: integer, index of epoch.
        :param logs: dictionary of logs.
        '''
        logs = logs or {}
        for callback in self.callbacks:
            callback.on_epoch_end(epoch, logs)

    def on_batch_begin(self, batch, logs=None):
        '''
        Called right before processing a batch.
        :param batch: integer, index of batch within the current epoch.
        :param logs: dictionary of logs.
        '''
        logs = logs or {}
        for callback in self.callbacks:
            callback.on_batch_begin(batch, logs)

    def on_batch_end(self, batch, logs=None):
        '''
        Called at the end of a batch.
        :param batch: integer, index of batch within the current epoch.
        :param logs: dictionary of logs.
        '''
        logs = logs or {}
        for callback in self.callbacks:
            callback.on_batch_end(batch, logs)

    def on_train_begin(self, logs=None):
        '''
        Called at the beginning of the training
        :param logs: dictionary of logs.
        '''
        logs = logs or {}
        for callback in self.callbacks:
            callback.on_train_begin(logs)

    def on_train_end(self, logs=None):
        '''
        Called at the end of training.
        :param logs: dictionary of logs.
        '''
        logs = logs or {}
        for callback in self.callbacks:
            callback.on_train_end(logs)


class Callback(object):
    def __init__(self):
        self.model = None

    def set_params(self, params):
        self.params = params

    def set_model(self, model):
        self.model = model

    def on_epoch_begin(self, epoch, logs=None):
        pass

    def on_epoch_end(self, epoch, logs=None):
        pass

    def on_batch_begin(self, batch, logs=None):
        pass

    def on_batch_end(self, batch, logs=None):
        pass

    def on_train_begin(self, logs=None):
        pass

    def on_train_end(self, logs=None):
        pass


# --------------------------------------------------------------------------------------------------------------------#
# Define Some Common Callbacks
# --------------------------------------------------------------------------------------------------------------------#
class AvgMetrics(Callback):
    """
    Records metrics over epochs by averaging over each batch.
    The metrics are calculated with a moving model
    """

    def on_epoch_begin(self, epoch, logs=None):
        self.seen = 0
        self.totals = {}
        self.metrics = ['loss'] + self.params['metrics']

    def on_batch_end(self, batch, logs=None):
        logs = logs or {}
        batch_size = logs.get('size', 1) or 1
        self.seen += batch_size

        for k, v in logs.items():
            if k in self.totals:
                self.totals[k] += v * batch_size
            else:
                self.totals[k] = v * batch_size

    def on_epoch_end(self, epoch, logs=None):
        if logs is not None:
            for k in self.metrics:
                if k in self.totals:
                    # Make value available to next callbacks.
                    logs[k] = self.totals[k] / self.seen


class ProgressBarLogger(Callback):
    '''
    TQDM progress bar that displays the running average of loss and other metrics.
    '''

    def __init__(self):
        super(ProgressBarLogger, self).__init__()

    def on_train_begin(self, logs=None):
        self.num_batches = self.params['num_batches']
        self.verbose = self.params['verbose']
        self.metrics = ['loss'] + self.params['metrics']

    def on_epoch_begin(self, epoch, logs=None):
        self.target = self.num_batches
        self.pbar = tqdm(total=self.target, desc="Epoch {}".format(epoch))
        self.seen = 0

    def on_batch_begin(self, batch, logs=None):
        self.log_values = {}

    def on_batch_end(self, batch, logs=None):
        logs = logs or {}
        self.seen += 1

        for k in self.metrics:
            if k in logs:
                self.log_values[k] = logs[k]

        # skip progbar update for the last batch;
        # will be handled by on_epoch_end.
        if self.verbose and self.seen < self.target:
            self.pbar.update(1)
            self.pbar.set_postfix(self.log_values)  # 设置进度条后面显示的信息

    def on_epoch_end(self, epoch, logs=None):
        # Update log values
        self.log_values = {}
        for k in self.metrics:
            if k in logs:
                self.log_values[k] = logs[k]

        if self.verbose:
            self.pbar.update(1)
            self.pbar.set_postfix(self.log_values)

        self.pbar.close()


class CSVLogger(Callback):
    """
    Callback that streams epoch results to a csv file.
    Supports all values that can be represented as a string,
    including 1D iterables such as np.ndarray.
    """

    def __init__(self, filename, separator=',', append=False):
        """
        Callback that streams epoch results to a csv file.
        Supports all values that can be represented as a string,
        including 1D iterables such as np.ndarray.
        :param filename: filename of the csv file, e.g. 'run/log.csv'.
        :param separator: string used to separate elements in the csv file.
        :param append: append if file exists (useful for continuing training). False: overwrite existing file,
        """
        self.sep = separator
        self.filename = filename
        self.append = append
        self.writer = None
        self.keys = None
        self.append_header = True
        self.file_flags = ''
        self._open_args = {'newline': '\n'}
        super(CSVLogger, self).__init__()

    def on_train_begin(self, logs=None):
        if self.append:
            if os.path.exists(self.filename):
                with open(self.filename, 'r' + self.file_flags) as f:
                    self.append_header = not bool(len(f.readline()))
            mode = 'a'
        else:
            mode = 'w'
        self.csv_file = io.open(self.filename,
                                mode + self.file_flags,
                                **self._open_args)

    def on_epoch_end(self, epoch, logs=None):
        logs = logs or {}

        def handle_value(k):
            is_zero_dim_ndarray = isinstance(k, np.ndarray) and k.ndim == 0
            if isinstance(k, str):
                return k
            elif isinstance(k, Iterable) and not is_zero_dim_ndarray:
                return '"[%s]"' % (', '.join(map(str, k)))
            else:
                return k

        if self.keys is None:
            self.keys = sorted(logs.keys())

        if not self.writer:
            class CustomDialect(csv.excel):
                delimiter = self.sep

            fieldnames = ['epoch'] + self.keys
            self.writer = csv.DictWriter(self.csv_file,
                                         fieldnames=fieldnames,
                                         dialect=CustomDialect)
            if self.append_header:
                self.writer.writeheader()

        row_dict = OrderedDict({'epoch': epoch})
        row_dict.update((key, handle_value(logs[key])) for key in self.keys)
        self.writer.writerow(row_dict)
        self.csv_file.flush()

    def on_train_end(self, logs=None):
        self.csv_file.close()
        self.writer = None


# class EvaluateMetrics(Callback):
#     """
#     Evaluates metrics on a dataset after every epoch.
#     """
#     def __init__(self, dataloader, prefix='val_', suffix=''):
#         """
#         Evaluates metrics on a dataset after every epoch.
#         :param dataloader: torch.DataLoader of the dataset on which the model will be evaluated
#         :param prefix: Prefix to prepend to the names of the metrics when they is logged. Defaults to 'val_' but can be changed if the model is to be evaluated on many datasets separately.
#         :param suffix: Suffix to append to the names of the metrics when they is logged.
#         """
#         super(EvaluateMetrics, self).__init__()
#         self.dataloader = dataloader
#         self.prefix = prefix
#         self.suffix = suffix
#     @staticmethod
#     def evaluate(model: Module, dataloader: DataLoader, prepare_batch: Callable, metrics: List[Union[str, Callable]],
#                  loss_fn: Callable = None, prefix: str = 'val_', suffix: str = ''):
#         """Evaluate a model on one or more metrics on a particular dataset
#
#         # Arguments
#             model: Model to evaluate
#             dataloader: Instance of torch.utils.data.DataLoader representing the dataset
#             prepare_batch: Callable to perform any desired preprocessing
#             metrics: List of metrics to evaluate the model with. Metrics must either be a named metric (see `metrics.py`) or
#                 a Callable that takes predictions and ground truth labels and returns a scalar value
#             loss_fn: Loss function to calculate over the dataset
#             prefix: Prefix to prepend to the name of each metric - used to identify the dataset. Defaults to 'val_' as
#                 it is typical to evaluate on a held-out validation dataset
#             suffix: Suffix to append to the name of each metric.
#         """
#         logs = {}
#         seen = 0
#         totals = {m: 0 for m in metrics}  # evaluation metrics' results
#         if loss_fn is not None:
#             totals['loss'] = 0
#         model.eval()
#         with torch.no_grad():
#             for batch in dataloader:
#                 x, y = prepare_batch(batch)
#                 y_pred = model(x)
#
#                 seen += x.shape[0]
#
#                 if loss_fn is not None:
#                     totals['loss'] += loss_fn(y_pred, y).item() * x.shape[0]
#
#                 for m in metrics:
#                     if isinstance(m, str):
#                         v = NAMED_METRICS[m](y, y_pred)
#                     else:
#                         # Assume metric is a callable function
#                         v = m(y, y_pred)
#
#                 totals[m] = v * x.shape[0]
#         for m in ['loss'] + metrics:
#             logs[prefix + m + suffix] = totals[m] / seen
#
#         return logs
#
#     def on_train_begin(self, logs=None):
#         self.metrics = self.params['metrics']
#         self.prepare_batch = self.params['prepare_batch']
#         self.loss_fn = self.params['loss_fn']
#
#     def on_epoch_end(self, epoch, logs=None):
#         logs = logs or {}
#         logs.update(
#             self.evaluate(self.model, self.dataloader, self.prepare_batch, self.metrics, self.loss_fn, self.prefix, self.suffix)
#         )

class ModelCheckpoint(Callback):
    """
    Save the model after every epoch.
    """

    def __init__(self, filepath, monitor='val_loss', verbose=0, save_best_only=False, mode='auto', period=1):
        """
        Save the model after every epoch.
        `filepath` can contain named formatting options, which will be filled the value of `epoch` and keys in `logs`
        (passed in `on_epoch_end`).

        For example: if `filepath` is `weights.{epoch:02d}-{val_loss:.2f}.hdf5`, then the model checkpoints will be saved
        with the epoch number and the validation loss in the filename.


        :param filepath: string, path to save the model file.
        :param monitor: quantity to monitor.
        :param verbose: verbosity mode, 0 or 1.
        :param save_best_only: if `save_best_only=True`,
            the latest best model according to
            the quantity monitored will not be overwritten.
        :param mode: one of {auto, min, max}.
            If `save_best_only=True`, the decision
            to overwrite the current save file is made
            based on either the maximization or the
            minimization of the monitored quantity. For `val_acc`,
            this should be `max`, for `val_loss` this should
            be `min`, etc. In `auto` mode, the direction is
            automatically inferred from the name of the monitored quantity.
        :param period: Interval (number of epochs) between checkpoints.
        """
        super(ModelCheckpoint, self).__init__()
        self.monitor = monitor
        self.verbose = verbose
        self.filepath = filepath
        self.save_best_only = save_best_only
        self.period = period
        self.epochs_since_last_save = 0

        if mode not in ['auto', 'min', 'max']:
            raise ValueError('Mode must be one of (auto, min, max).')

        if mode == 'min':
            self.monitor_op = np.less
            self.best = np.Inf
        elif mode == 'max':
            self.monitor_op = np.greater
            self.best = -np.Inf
        else:
            if 'acc' in self.monitor or self.monitor.startswith('fmeasure'):
                self.monitor_op = np.greater
                self.best = -np.Inf
            else:
                self.monitor_op = np.less

        self.best = np.Inf

    def on_epoch_end(self, epoch, logs=None):
        logs = logs or {}
        self.epochs_since_last_save += 1
        if self.epochs_since_last_save >= self.period:
            self.epochs_since_last_save = 0
            filepath = self.filepath.format(epoch=epoch + 1, **logs)
            if self.save_best_only:
                current = logs.get(self.monitor)
                if current is None:
                    warnings.warn('Can save best model only with %s available, '
                                  'skipping.' % (self.monitor), RuntimeWarning)
                else:
                    if self.monitor_op(current, self.best):
                        if self.verbose > 0:
                            print('\nEpoch %05d: %s improved from %0.5f to %0.5f,'
                                  ' saving model to %s'
                                  % (epoch + 1, self.monitor, self.best,
                                     current, filepath))
                        self.best = current
                        torch.save(self.model.state_dict(), filepath)
                    else:
                        if self.verbose > 0:
                            print('\nEpoch %05d: %s did not improve from %0.5f' %
                                  (epoch + 1, self.monitor, self.best))
            else:
                if self.verbose > 0:
                    print('\nEpoch %05d: saving model to %s' % (epoch + 1, filepath))
                torch.save(self.model.state_dict(), filepath)


class LearningRateScheduler(Callback):
    """
    Learning rate scheduler.
    """

    def __init__(self, schedule, verbose=0):
        """
        Learning rate scheduler.
        :param schedule: schedule: a function that takes an epoch index as input
            (integer, indexed from 0) and current learning rate
            and returns a new learning rate as output (float).
        :param verbose: int. 0: quiet, 1: update messages.
        """
        super(LearningRateScheduler, self).__init__()
        self.schedule = schedule
        self.verbose = verbose

    def on_train_begin(self, logs=None):
        self.optimizer = self.params['optimizer']

    def on_epoch_begin(self, epoch, logs=None):
        lrs = [self.schedule(epoch, param_group['lr']) for param_group in self.optimizer.param_groups]

        if not all(isinstance(lr, (float, np.float32, np.float64)) for lr in lrs):
            raise ValueError('The output of the "schedule" function '
                             'should be float.')
        self.set_lr(epoch, lrs)

    def on_epoch_end(self, epoch, logs=None):
        logs = logs or {}
        if len(self.optimizer.param_groups) == 1:
            logs['lr'] = self.optimizer.param_groups[0]['lr']
        else:
            for i, param_group in enumerate(self.optimizer.param_groups):
                logs['lr_{}'.format(i)] = param_group['lr']

    def set_lr(self, epoch, lrs):
        for i, param_group in enumerate(self.optimizer.param_groups):
            new_lr = lrs[i]
            param_group['lr'] = new_lr
            if self.verbose:
                print('Epoch {:5d}: setting learning rate'
                      ' of group {} to {:.4e}.'.format(epoch, i, new_lr))


class ReduceLROnPlateau(Callback):
    """
    Reduce learning rate when a metric has stopped improving.
    """
    def __init__(self, monitor='val_loss', factor=0.1, patience=10,
                 verbose=0, mode='auto', min_delta=1e-4, cooldown=0, min_lr=0,
                 **kwargs):
        """
        Reduce learning rate when a metric has stopped improving.

        Models often benefit from reducing the learning rate by a factor
        of 2-10 once learning stagnates. This callback monitors a
        quantity and if no improvement is seen for a 'patience' number
        of epochs, the learning rate is reduced.
        :param monitor: quantity to be monitored.
        :param factor: factor by which the learning rate will be reduced. new_lr = lr * factor
        :param patience: number of epochs with no improvement after which learning rate will be reduced.
        :param verbose: int. 0: quiet, 1: update messages.
        :param mode: one of {auto, min, max}. In `min` mode, lr will be reduced when the quantity
            monitored has stopped decreasing; in `max` mode it will be reduced when the quantity
            monitored has stopped increasing; in `auto` mode, the direction is automatically inferred
            from the name of the monitored quantity.
        :param min_delta: threshold for measuring the new optimum, to only focus on significant changes.
        :param cooldown: number of epochs to wait before resuming normal operation after lr has been reduced.
        :param min_lr: lower bound on the learning rate.
        """
        super(ReduceLROnPlateau, self).__init__()

        self.monitor = monitor
        if factor >= 1.0:
            raise ValueError('ReduceLROnPlateau does not support a factor >= 1.0.')
        self.factor = factor
        self.min_lr = min_lr
        self.min_delta = min_delta
        self.patience = patience
        self.verbose = verbose
        self.cooldown = cooldown
        self.cooldown_counter = 0  # Cooldown counter.
        self.wait = 0
        self.best = 0
        if mode not in ['auto', 'min', 'max']:
            raise ValueError('Mode must be one of (auto, min, max).')
        self.mode = mode
        self.monitor_op = None

        self._reset()

    def _reset(self):
        """Resets wait counter and cooldown counter.
        """
        if (self.mode == 'min' or
                (self.mode == 'auto' and 'acc' not in self.monitor)):
            self.monitor_op = lambda a, b: np.less(a, b - self.min_delta)
            self.best = np.Inf
        else:
            self.monitor_op = lambda a, b: np.greater(a, b + self.min_delta)
            self.best = -np.Inf
        self.cooldown_counter = 0
        self.wait = 0

    def on_train_begin(self, logs=None):
        self.optimizer = self.params['optimizer']
        self.min_lrs = [self.min_lr] * len(self.optimizer.param_groups)
        self._reset()

    def on_epoch_end(self, epoch, logs=None):
        logs = logs or {}
        if len(self.optimizer.param_groups) == 1:
            logs['lr'] = self.optimizer.param_groups[0]['lr']
        else:
            for i, param_group in enumerate(self.optimizer.param_groups):
                logs['lr_{}'.format(i)] = param_group['lr']

        current = logs.get(self.monitor)

        if self.in_cooldown():
            self.cooldown_counter -= 1
            self.wait = 0

        if self.monitor_op(current, self.best):
            self.best = current
            self.wait = 0
        elif not self.in_cooldown():
            self.wait += 1
            if self.wait >= self.patience:
                self._reduce_lr(epoch)
                self.cooldown_counter = self.cooldown
                self.wait = 0

    def _reduce_lr(self, epoch):
        for i, param_group in enumerate(self.optimizer.param_groups):
            old_lr = float(param_group['lr'])
            new_lr = max(old_lr * self.factor, self.min_lrs[i])
            if old_lr - new_lr > self.min_delta:
                param_group['lr'] = new_lr
                if self.verbose:
                    print('Epoch {:5d}: reducing learning rate'
                          ' of group {} to {:.4e}.'.format(epoch, i, new_lr))

    def in_cooldown(self):
        return self.cooldown_counter > 0
